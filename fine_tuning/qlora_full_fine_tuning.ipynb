{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning both vision and language parts","metadata":{}},{"cell_type":"markdown","source":"### There are problems with putting multimodal data in the model. Necessary to use special processor class object, but not important in our case","metadata":{}},{"cell_type":"markdown","source":"### Installing requirements","metadata":{}},{"cell_type":"code","source":"!pip install -q \\\n    torch \\\n    numpy \\\n    tqdm \\\n    torchvision \\\n    transformers \\\n    datasets \\\n    peft \\\n    accelerate \\\n    matplotlib \\\n    git+https://github.com/openai/CLIP.git \\\n    git+https://github.com/salaniz/pycocoevalcap\n!pip install -q --upgrade bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:54:42.669576Z","iopub.execute_input":"2025-05-21T19:54:42.670458Z","iopub.status.idle":"2025-05-21T19:56:51.294177Z","shell.execute_reply.started":"2025-05-21T19:54:42.670432Z","shell.execute_reply":"2025-05-21T19:56:51.293161Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Necessary imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\nimport bitsandbytes as bnb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:56:51.295964Z","iopub.execute_input":"2025-05-21T19:56:51.296266Z","iopub.status.idle":"2025-05-21T19:57:15.567852Z","shell.execute_reply.started":"2025-05-21T19:56:51.296236Z","shell.execute_reply":"2025-05-21T19:57:15.567297Z"}},"outputs":[{"name":"stderr","text":"2025-05-21 19:57:04.632412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747857424.822280      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747857424.877128      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Defining variables","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"OpenGVLab/InternVL2_5-4B\"\nDATASET_NAME = \"d0rj/LLaVA-OneVision-Data-ru\"\nDATASET_SUBDIR = \"ureader_cap\"\nOUTPUT_DIR = \"./finetuned_model\"\nBATCH_SIZE = 1\nEPOCHS = 3\nLR = 2e-4  # learning rate\nMAX_LENGTH = 512\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = 448   #  for load_image()\nMAX_NUM = 6       # max amount of tiles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:57:15.568536Z","iopub.execute_input":"2025-05-21T19:57:15.568994Z","iopub.status.idle":"2025-05-21T19:57:15.573661Z","shell.execute_reply.started":"2025-05-21T19:57:15.568967Z","shell.execute_reply":"2025-05-21T19:57:15.572929Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Uploading model and tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# Model 4bit с bnb\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map='auto',\n    quantization_config=quantization_config,\n    trust_remote_code=True\n)\n\n# Preparing model for QLoRA\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:57:15.575523Z","iopub.execute_input":"2025-05-21T19:57:15.575803Z","iopub.status.idle":"2025-05-21T19:57:54.980175Z","shell.execute_reply.started":"2025-05-21T19:57:15.575779Z","shell.execute_reply":"2025-05-21T19:57:54.979649Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd76fc4378c04fb2853936323aa38477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d236e43189f6454fb3dcf229484bb8cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b72b74a3de421ab17f152bc95136cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/790 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d263df97e8d491ba03e844c68176b68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"994ec40a14104fb4ab7cce5495402bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf40793a2a34e14965aaca90158a6cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_internvl_chat.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87143965c983493aaa3b25bb4acf48d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc4ec0c27ca484c88e667bcb383bc01"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- configuration_internvl_chat.py\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_internvl_chat.py:   0%|          | 0.00/15.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de93a9e1ddf9427abe8e474aab10b870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_intern_vit.py:   0%|          | 0.00/18.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7e626d2fee46849a44f87b82caca79"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"conversation.py:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a396fbe130942bb899f2cb18f68ba7c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- modeling_internvl_chat.py\n- modeling_intern_vit.py\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"FlashAttention2 is not installed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/71.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c7ae3a9725d407082d91999c0721437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a67afd786f34c2fa141653311caf74b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"828cbd8622e145369351941a5732f4cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8b48586aca446a880ccf9c8b94fc45"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caad5d9faafa4a14b90681bec2700e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"091efe569f214157bd04fdeafe8334e3"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### LoRA configuration","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],  # for LLMs\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:57:54.980851Z","iopub.execute_input":"2025-05-21T19:57:54.981044Z","iopub.status.idle":"2025-05-21T19:57:55.121657Z","shell.execute_reply.started":"2025-05-21T19:57:54.981026Z","shell.execute_reply":"2025-05-21T19:57:55.121095Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,686,400 || all params: 3,716,324,352 || trainable%: 0.0992\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Preparing the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(DATASET_NAME, DATASET_SUBDIR)\n\ntrain_data = dataset[\"train\"].select(range(1000))\n\n# Learned normalization for images\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T19:57:55.122343Z","iopub.execute_input":"2025-05-21T19:57:55.122636Z","iopub.status.idle":"2025-05-21T20:02:26.005543Z","shell.execute_reply.started":"2025-05-21T19:57:55.122618Z","shell.execute_reply":"2025-05-21T20:02:26.004698Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/37.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd0a6a13b0b4343a0f59d91db08f6ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"586fc7021445484c9dd7404f807ed4a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/19 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026ace438da84975aaacd5cd3d822443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00000-of-00019.parquet:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e9eb1a955a4159ba54db725ac53c43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00001-of-00019.parquet:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ae91258a9244d2ac3eeaccc41e29ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00002-of-00019.parquet:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cfbcd41d9544f7e8c81d11d20910b0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00003-of-00019.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee08baab55964c95ab14786920825b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00004-of-00019.parquet:   0%|          | 0.00/112M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06033b2d76944f987c3a98aa28561e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00005-of-00019.parquet:   0%|          | 0.00/125M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1bf62dd7f5241eab708fb2f0ac6daf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00006-of-00019.parquet:   0%|          | 0.00/116M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a410ccf2ee470580c01501a4aea54f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00007-of-00019.parquet:   0%|          | 0.00/130M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7022982285374a80834ae915e5dbda17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00008-of-00019.parquet:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73184373d3bd4be68c833b73dee2145d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00009-of-00019.parquet:   0%|          | 0.00/124M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae57009a560462ca382c208a556a418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00010-of-00019.parquet:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51fd5f9623024a7c86f4c034cf84592b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00011-of-00019.parquet:   0%|          | 0.00/128M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c74e27e63ffb45d4ad44150b47a17ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00012-of-00019.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5afda9b335d540169357c70a6c0e26d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00013-of-00019.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b91adb8db2f4a3597ab2c4b10e060a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00014-of-00019.parquet:   0%|          | 0.00/108M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e5505ee2704b3286be3dfb43130aeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00015-of-00019.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16536b75cb4d4a8d9f79206045c845a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00016-of-00019.parquet:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80f66a8bb2594f10bdf6e9644d09cad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00017-of-00019.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33f599521ba4cd39f80e9d1775a8fbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00018-of-00019.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d43611a9354748e48f805ef410cd05c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/91434 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"422b2b3f0738475cb382961bfae11584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f5784cc4fa4b9c98e99aa5187661e2"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### Transformations pipeline","metadata":{}},{"cell_type":"code","source":"def build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:26.006441Z","iopub.execute_input":"2025-05-21T20:02:26.007209Z","iopub.status.idle":"2025-05-21T20:02:26.012383Z","shell.execute_reply.started":"2025-05-21T20:02:26.007178Z","shell.execute_reply":"2025-05-21T20:02:26.011473Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Finding the most appropriate aspect ratio for an image","metadata":{}},{"cell_type":"code","source":"def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:26.013618Z","iopub.execute_input":"2025-05-21T20:02:26.013886Z","iopub.status.idle":"2025-05-21T20:02:26.929428Z","shell.execute_reply.started":"2025-05-21T20:02:26.013863Z","shell.execute_reply":"2025-05-21T20:02:26.928391Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Image preprocessing","metadata":{}},{"cell_type":"code","source":"def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:26.930494Z","iopub.execute_input":"2025-05-21T20:02:26.930807Z","iopub.status.idle":"2025-05-21T20:02:27.644485Z","shell.execute_reply.started":"2025-05-21T20:02:26.930774Z","shell.execute_reply":"2025-05-21T20:02:27.643654Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Loading images","metadata":{}},{"cell_type":"code","source":"def load_image(image_file, input_size=448, max_num=12):\n    image = image_file.convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:27.648462Z","iopub.execute_input":"2025-05-21T20:02:27.648697Z","iopub.status.idle":"2025-05-21T20:02:28.769396Z","shell.execute_reply.started":"2025-05-21T20:02:27.648681Z","shell.execute_reply":"2025-05-21T20:02:28.768485Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### General preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    input_ids_list = []\n    attention_mask_list = []\n    pixel_values_list = []\n    labels_list = []\n\n    for convs, image in zip(examples[\"conversations\"], examples[\"image\"]):\n        full_text = \"\"\n        label_mask = []\n\n        for turn in convs:\n            if turn[\"from\"] == \"human\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [0] * len(tokenizer.tokenize(text + \"\\n\"))\n            elif turn[\"from\"] == \"gpt\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [1] * len(tokenizer.tokenize(text + \"\\n\"))\n\n        tokenized = tokenizer(\n            full_text,\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        input_ids = tokenized[\"input_ids\"].squeeze(0)        # tensor\n        attention_mask = tokenized[\"attention_mask\"].squeeze(0)  # tensor\n\n        label_mask = label_mask[:MAX_LENGTH]\n        label_mask += [0] * (MAX_LENGTH - len(label_mask))\n\n        labels_tensor = input_ids.clone()  # cloning tensor\n        for i, mask in enumerate(label_mask):\n            if mask == 0:\n                labels_tensor[i] = -100\n\n        pixel_values = load_image(image, input_size=INPUT_SIZE, max_num=MAX_NUM)\n        pixel_values = [p.tolist() for p in pixel_values]\n\n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        pixel_values_list.append(pixel_values)\n        labels_list.append(labels_tensor)\n\n    batch_input_ids = torch.stack(input_ids_list)\n    batch_attention_mask = torch.stack(attention_mask_list)\n    # pixel_values — list\n    batch_labels = torch.stack(labels_list)\n\n    return {\n        \"input_ids\": batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"pixel_values\": pixel_values_list,\n        \"labels\": batch_labels,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:28.770174Z","iopub.execute_input":"2025-05-21T20:02:28.770519Z","iopub.status.idle":"2025-05-21T20:02:29.646390Z","shell.execute_reply.started":"2025-05-21T20:02:28.770489Z","shell.execute_reply":"2025-05-21T20:02:29.645555Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"processed_dataset = train_data.map(\n    preprocess_function,\n    batched=True,\n    batch_size=16,\n    remove_columns=train_data.column_names,\n    load_from_cache_file=False\n)\n\n# Splitting on training/validation\nsplit = processed_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:02:29.647526Z","iopub.execute_input":"2025-05-21T20:02:29.647877Z","iopub.status.idle":"2025-05-21T20:14:46.767824Z","shell.execute_reply.started":"2025-05-21T20:02:29.647849Z","shell.execute_reply":"2025-05-21T20:14:46.766863Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a0e6af9dd744ff48298a13201332222"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### Collating","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids_list = []\n    attention_mask_list = []\n    labels_list = []\n    pixel_values_list = []\n\n    for sample in batch:\n        input_ids_list.append(torch.tensor(sample['input_ids'], dtype=torch.long))\n        attention_mask_list.append(torch.tensor(sample['attention_mask'], dtype=torch.long))\n        labels_list.append(torch.tensor(sample['labels'], dtype=torch.long))\n\n        pv = torch.stack([torch.tensor(p) for p in sample['pixel_values']])\n        pixel_values_list.append(pv)\n\n    return {\n        'input_ids': torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id),\n        'attention_mask': torch.nn.utils.rnn.pad_sequence(attention_mask_list, batch_first=True, padding_value=0),\n        'labels': torch.nn.utils.rnn.pad_sequence(labels_list, batch_first=True, padding_value=-100),\n        'pixel_values': torch.stack(pixel_values_list)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:14:46.769911Z","iopub.execute_input":"2025-05-21T20:14:46.770135Z","iopub.status.idle":"2025-05-21T20:14:46.777084Z","shell.execute_reply.started":"2025-05-21T20:14:46.770119Z","shell.execute_reply":"2025-05-21T20:14:46.776369Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Batching","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:14:46.777993Z","iopub.execute_input":"2025-05-21T20:14:46.778265Z","iopub.status.idle":"2025-05-21T20:14:49.558452Z","shell.execute_reply.started":"2025-05-21T20:14:46.778224Z","shell.execute_reply":"2025-05-21T20:14:49.557471Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Defining metrics","metadata":{}},{"cell_type":"code","source":"from pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nimport clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:14:49.559251Z","iopub.execute_input":"2025-05-21T20:14:49.559518Z","iopub.status.idle":"2025-05-21T20:14:49.728542Z","shell.execute_reply.started":"2025-05-21T20:14:49.559498Z","shell.execute_reply":"2025-05-21T20:14:49.727953Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"cider_scorer = Cider()\nspice_scorer = Spice()\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:14:49.729293Z","iopub.execute_input":"2025-05-21T20:14:49.729565Z","iopub.status.idle":"2025-05-21T20:16:10.565861Z","shell.execute_reply.started":"2025-05-21T20:14:49.729544Z","shell.execute_reply":"2025-05-21T20:16:10.565069Z"}},"outputs":[{"name":"stdout","text":"Downloading stanford-corenlp-3.6.0 for SPICE ...\nProgress: 384.5M / 384.5M (100.0%)\nExtracting stanford-corenlp-3.6.0 ...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 126MiB/s]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Computing CLIPScore","metadata":{}},{"cell_type":"code","source":"def compute_clip_score(cand_sentences, ref_sentences, batch_size=16):\n    scores = []\n    for i in range(0, len(cand_sentences), batch_size):\n        batch_cand = cand_sentences[i:i+batch_size]\n        batch_ref = ref_sentences[i:i+batch_size]\n        cand_inputs = clip.tokenize(batch_cand).to(DEVICE)\n        ref_inputs = clip.tokenize(batch_ref).to(DEVICE)\n        cand_feats = clip_model.encode_text(cand_inputs)\n        ref_feats = clip_model.encode_text(ref_inputs)\n        cand_feats = cand_feats / cand_feats.norm(dim=-1, keepdim=True)\n        ref_feats = ref_feats / ref_feats.norm(dim=-1, keepdim=True)\n        scores.extend((cand_feats * ref_feats).sum(dim=-1).tolist())\n    return np.mean(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:10.567096Z","iopub.execute_input":"2025-05-21T20:16:10.567341Z","iopub.status.idle":"2025-05-21T20:16:10.572598Z","shell.execute_reply.started":"2025-05-21T20:16:10.567299Z","shell.execute_reply":"2025-05-21T20:16:10.572041Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Optimizer, scheduler, accelerator","metadata":{}},{"cell_type":"code","source":"# Optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nnum_training_steps = EPOCHS * len(train_loader)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# Accelerator\naccelerator = Accelerator()\nmodel, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_loader, val_loader, lr_scheduler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:10.573181Z","iopub.execute_input":"2025-05-21T20:16:10.573427Z","iopub.status.idle":"2025-05-21T20:16:13.529909Z","shell.execute_reply.started":"2025-05-21T20:16:10.573411Z","shell.execute_reply":"2025-05-21T20:16:13.528958Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Final preparations","metadata":{}},{"cell_type":"markdown","source":"### Generating text for validation","metadata":{}},{"cell_type":"code","source":"def generate_text(input_ids, attention_mask):\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=MAX_LENGTH,\n        num_beams=1,\n        early_stopping=True,\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:13.530796Z","iopub.execute_input":"2025-05-21T20:16:13.531059Z","iopub.status.idle":"2025-05-21T20:16:13.534954Z","shell.execute_reply.started":"2025-05-21T20:16:13.531035Z","shell.execute_reply":"2025-05-21T20:16:13.534350Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"markdown","source":"Here is the problem","metadata":{}},{"cell_type":"code","source":"train_losses = []\nval_losses = []\nspice_scores = []\ncider_scores = []\nclip_scores = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:13.535694Z","iopub.execute_input":"2025-05-21T20:16:13.535950Z","iopub.status.idle":"2025-05-21T20:16:13.549684Z","shell.execute_reply.started":"2025-05-21T20:16:13.535930Z","shell.execute_reply":"2025-05-21T20:16:13.549105Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader):\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Валидация\n    model.eval()\n    total_val_loss = 0\n    preds = []\n    refs = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            # Генерируем предсказания для метрик\n            generated_texts = generate_text(batch[\"input_ids\"], batch[\"attention_mask\"])\n            refs.extend(tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True))\n            preds.extend(generated_texts)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    # Подсчет SPICE и CIDEr (pycocoevalcap работает со словарями)\n    # Формат: [{'image_id': i, 'caption': 'text'}, ...]\n    res = [{\"image_id\": i, \"caption\": p} for i, p in enumerate(preds)]\n    gts = {i: [refs[i]] for i in range(len(refs))}\n\n    cider_score, _ = cider_scorer.compute_score(gts, res)\n    spice_score, _ = spice_scorer.compute_score(gts, res)\n\n    cider_scores.append(cider_score)\n    spice_scores.append(spice_score)\n\n    # CLIPScore\n    clip_score = compute_clip_score(preds, refs)\n    clip_scores.append(clip_score)\n\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | SPICE: {spice_score:.4f} | CIDEr: {cider_score:.4f} | CLIPScore: {clip_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:13.550463Z","iopub.execute_input":"2025-05-21T20:16:13.550753Z","iopub.status.idle":"2025-05-21T20:16:18.247845Z","shell.execute_reply.started":"2025-05-21T20:16:13.550729Z","shell.execute_reply":"2025-05-21T20:16:18.246764Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/900 [00:04<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1814084295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: InternVLChatModel.forward() got an unexpected keyword argument 'inputs_embeds'"],"ename":"TypeError","evalue":"InternVLChatModel.forward() got an unexpected keyword argument 'inputs_embeds'","output_type":"error"}],"execution_count":22},{"cell_type":"markdown","source":"### Saving model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Model is saved: {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:18.248549Z","iopub.status.idle":"2025-05-21T20:16:18.248878Z","shell.execute_reply.started":"2025-05-21T20:16:18.248705Z","shell.execute_reply":"2025-05-21T20:16:18.248720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graphical visualization","metadata":{}},{"cell_type":"code","source":"epochs = np.arange(1, EPOCHS+1)\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, spice_scores, label=\"SPICE\", color=\"orange\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"SPICE\")\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, cider_scores, label=\"CIDEr\", color=\"green\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CIDEr\")\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, clip_scores, label=\"CLIPScore\", color=\"red\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CLIPScore\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T20:16:18.250414Z","iopub.status.idle":"2025-05-21T20:16:18.250706Z","shell.execute_reply.started":"2025-05-21T20:16:18.250548Z","shell.execute_reply":"2025-05-21T20:16:18.250564Z"}},"outputs":[],"execution_count":null}]}