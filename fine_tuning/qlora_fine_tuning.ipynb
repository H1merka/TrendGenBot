{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing requirements","metadata":{}},{"cell_type":"code","source":"!pip install torch\n!pip install numpy\n!pip install tqdm\n!pip install torchvision\n!pip install transformers\n!pip install datasets\n!pip install peft\n!pip install accelerate\n!pip install --upgrade bitsandbytes\n!pip install matplotlib\n!pip install git+https://github.com/openai/CLIP.git\n!pip install git+https://github.com/salaniz/pycocoevalcap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T09:51:58.322505Z","iopub.execute_input":"2025-05-21T09:51:58.322963Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Necessary imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\nimport bitsandbytes as bnb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining variables","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"OpenGVLab/InternVL2_5-4B\"\nDATASET_NAME = \"d0rj/LLaVA-OneVision-Data-ru\"\nDATASET_SUBDIR = \"ureader_cap\"\nOUTPUT_DIR = \"./finetuned_model\"\nBATCH_SIZE = 1  # для P100 - маленький батч, чтобы не упереться в VRAM\nEPOCHS = 3\nLR = 2e-4\nMAX_LENGTH = 512\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nINPUT_SIZE = 448   # для load_image\nMAX_NUM = 6       # макс количество тайлов","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Uploading model and tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# модель 4bit с bnb\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map='auto',\n    quantization_config=quantization_config,\n    trust_remote_code=True\n)\n\n# Подготовка модели для LoRA QLoRA\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Конфиг LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Пример для трансформеров, можно подстроить под модель\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(DATASET_NAME, DATASET_SUBDIR)\n\ntrain_data = dataset[\"train\"].select(range(3000))\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def load_image(image_file, input_size=448, max_num=12):\n    image = image_file.convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    input_ids_list = []\n    attention_mask_list = []\n    pixel_values_list = []\n    labels_list = []\n\n    for convs, image in zip(examples[\"conversations\"], examples[\"image\"]):\n        full_text = \"\"\n        label_mask = []\n\n        for turn in convs:\n            if turn[\"from\"] == \"human\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [0] * len(tokenizer.tokenize(text + \"\\n\"))\n            elif turn[\"from\"] == \"gpt\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [1] * len(tokenizer.tokenize(text + \"\\n\"))\n\n        tokenized = tokenizer(\n            full_text,\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"  # Возвращаем тензоры сразу\n        )\n\n        input_ids = tokenized[\"input_ids\"].squeeze(0)        # [MAX_LENGTH]\n        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n\n        label_mask = label_mask[:MAX_LENGTH]\n        label_mask += [0] * (MAX_LENGTH - len(label_mask))\n\n        labels = input_ids.clone().tolist()\n        for i, mask in enumerate(label_mask):\n            if mask == 0:\n                labels[i] = -100\n        labels = torch.tensor(labels)\n\n        pixel_values = load_image(image, input_size=INPUT_SIZE, max_num=MAX_NUM)\n\n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        pixel_values_list.append(pixel_values)\n        labels_list.append(labels)\n\n    # Конвертируем списки тензоров в батч-тензоры\n    batch_input_ids = torch.stack(input_ids_list)\n    batch_attention_mask = torch.stack(attention_mask_list)\n    # pixel_values — разной размерности по num_tiles, нельзя stack сразу, пусть остаются списком\n    batch_labels = torch.stack(labels_list)\n\n    return {\n        \"input_ids\": batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"pixel_values\": pixel_values_list,  # оставляем списком, паддим в collate_fn\n        \"labels\": batch_labels,\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Splitting into training and test samples","metadata":{}},{"cell_type":"code","source":"# Передаём конкретные названия колонок для удаления\nremove_columns = train_data.column_names\n\n# Применяем map\nprocessed_dataset = train_data.map(\n    preprocess_function,\n    batched=True,\n    batch_size=16,\n    remove_columns=remove_columns,\n    load_from_cache_file=False\n)\n\n# Разбиваем на train/val\nsplit = processed_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])         # теперь все тензоры одинакового размера\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    pixel_values_list = [item[\"pixel_values\"] for item in batch]\n    max_tiles = max(pv.size(0) for pv in pixel_values_list)\n\n    padded_pixel_values = []\n    for pv in pixel_values_list:\n        pad_len = max_tiles - pv.size(0)\n        if pad_len > 0:\n            pad_tensor = torch.zeros((pad_len, *pv.shape[1:]), dtype=pv.dtype)\n            padded = torch.cat([pv, pad_tensor], dim=0)\n        else:\n            padded = pv\n        padded_pixel_values.append(padded)\n    pixel_values = torch.stack(padded_pixel_values)\n\n    labels = torch.stack([item[\"labels\"] for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"pixel_values\": pixel_values,\n        \"labels\": labels\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining metrics","metadata":{}},{"cell_type":"code","source":"from pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nimport clip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cider_scorer = Cider()\nspice_scorer = Spice()\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Функция для вычисления CLIPScore\ndef compute_clip_score(cand_sentences, ref_sentences, batch_size=16):\n    scores = []\n    for i in range(0, len(cand_sentences), batch_size):\n        batch_cand = cand_sentences[i:i+batch_size]\n        batch_ref = ref_sentences[i:i+batch_size]\n        cand_inputs = clip.tokenize(batch_cand).to(DEVICE)\n        ref_inputs = clip.tokenize(batch_ref).to(DEVICE)\n        cand_feats = clip_model.encode_text(cand_inputs)\n        ref_feats = clip_model.encode_text(ref_inputs)\n        cand_feats = cand_feats / cand_feats.norm(dim=-1, keepdim=True)\n        ref_feats = ref_feats / ref_feats.norm(dim=-1, keepdim=True)\n        scores.extend((cand_feats * ref_feats).sum(dim=-1).tolist())\n    return np.mean(scores)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimizer, scheduler, accelerator","metadata":{}},{"cell_type":"code","source":"# --- Оптимизатор и scheduler ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nnum_training_steps = EPOCHS * len(train_loader)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# --- Accelerator для удобства ---\naccelerator = Accelerator()\nmodel, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_loader, val_loader, lr_scheduler\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final preparations","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# --- Функция генерации для оценки (beam search для валид)\ndef generate_text(input_ids, attention_mask):\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=MAX_LENGTH,\n        num_beams=1,\n        early_stopping=True,\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# --- Тренировочный цикл ---\ntrain_losses = []\nval_losses = []\nspice_scores = []\ncider_scores = []\nclip_scores = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader):\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Валидация\n    model.eval()\n    total_val_loss = 0\n    preds = []\n    refs = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            # Генерируем предсказания для метрик\n            generated_texts = generate_text(batch[\"input_ids\"], batch[\"attention_mask\"])\n            refs.extend(tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True))\n            preds.extend(generated_texts)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    # Подсчет SPICE и CIDEr (pycocoevalcap работает со словарями)\n    # Формат: [{'image_id': i, 'caption': 'text'}, ...]\n    res = [{\"image_id\": i, \"caption\": p} for i, p in enumerate(preds)]\n    gts = {i: [refs[i]] for i in range(len(refs))}\n\n    cider_score, _ = cider_scorer.compute_score(gts, res)\n    spice_score, _ = spice_scorer.compute_score(gts, res)\n\n    cider_scores.append(cider_score)\n    spice_scores.append(spice_score)\n\n    # CLIPScore\n    clip_score = compute_clip_score(preds, refs)\n    clip_scores.append(clip_score)\n\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | SPICE: {spice_score:.4f} | CIDEr: {cider_score:.4f} | CLIPScore: {clip_score:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving model","metadata":{}},{"cell_type":"code","source":"# --- Сохраняем модель ---\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Модель сохранена в {OUTPUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graphical visualization","metadata":{}},{"cell_type":"code","source":"epochs = np.arange(1, EPOCHS+1)\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, spice_scores, label=\"SPICE\", color=\"orange\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"SPICE\")\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, cider_scores, label=\"CIDEr\", color=\"green\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CIDEr\")\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, clip_scores, label=\"CLIPScore\", color=\"red\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CLIPScore\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}