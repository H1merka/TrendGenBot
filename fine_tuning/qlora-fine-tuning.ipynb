{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing requirements","metadata":{}},{"cell_type":"code","source":"!pip install -q \\\n  torch \\\n  numpy \\\n  tqdm \\\n  torchvision \\\n  pillow \\\n  transformers \\\n  datasets \\\n  peft \\\n  bitsandbytes \\\n  accelerate \\\n  evaluate \\\n  sentence_transformers \\\n  matplotlib \\\n  nltk \\\n  git+https://github.com/openai/CLIP.git \\\n  git+https://github.com/salaniz/pycocoevalcap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:36:32.473246Z","iopub.execute_input":"2025-05-19T14:36:32.474004Z","iopub.status.idle":"2025-05-19T14:38:12.258569Z","shell.execute_reply.started":"2025-05-19T14:36:32.473970Z","shell.execute_reply":"2025-05-19T14:38:12.257898Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Necessary imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, get_scheduler, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nimport bitsandbytes as bnb\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\nfrom PIL import Image\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:38:12.260090Z","iopub.execute_input":"2025-05-19T14:38:12.260363Z","iopub.status.idle":"2025-05-19T14:38:39.593533Z","shell.execute_reply.started":"2025-05-19T14:38:12.260335Z","shell.execute_reply":"2025-05-19T14:38:39.592731Z"}},"outputs":[{"name":"stderr","text":"2025-05-19 14:38:26.824784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747665507.041100      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747665507.107246      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:38:39.594473Z","iopub.execute_input":"2025-05-19T14:38:39.595115Z","iopub.status.idle":"2025-05-19T14:38:39.712601Z","shell.execute_reply.started":"2025-05-19T14:38:39.595093Z","shell.execute_reply":"2025-05-19T14:38:39.711805Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### Defining variables","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"OpenGVLab/InternVL2_5-4B\"\nDATASET_NAME = \"d0rj/LLaVA-OneVision-Data-ru\"\nDATASET_SUBDIR = \"ureader_cap\"\nOUTPUT_DIR = \"./finetuned_model\"\nBATCH_SIZE = 1  # для P100 - маленький батч, чтобы не упереться в VRAM\nEPOCHS = 3\nLR = 2e-4\nMAX_LENGTH = 512\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nINPUT_SIZE = 448   # для load_image\nMAX_NUM = 6       # макс количество тайлов","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:38:39.714444Z","iopub.execute_input":"2025-05-19T14:38:39.714690Z","iopub.status.idle":"2025-05-19T14:38:40.153174Z","shell.execute_reply.started":"2025-05-19T14:38:39.714671Z","shell.execute_reply":"2025-05-19T14:38:40.152301Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Uploading model and tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# модель 4bit с bnb\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    quantization_config=quantization_config,\n    trust_remote_code=True\n)\n\n# Подготовка модели для LoRA QLoRA\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:38:40.153911Z","iopub.execute_input":"2025-05-19T14:38:40.154141Z","iopub.status.idle":"2025-05-19T14:39:30.215420Z","shell.execute_reply.started":"2025-05-19T14:38:40.154122Z","shell.execute_reply":"2025-05-19T14:39:30.214413Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3042c69f544b4d8b85001f46d2ef62b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638b74d70b70402f9e810b2e20095d17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00be35924614152ad8247ab51eec87a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/790 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0cf43a2cff84be4bf7379e14cbb4b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a64d326732944d2869485a6ce9b5070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65bf6031b05d4b17a778a4ccb254309e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_internvl_chat.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ec45339ec44c07b3b5cfbd473cff0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6425776ad8b34904adf92d3201d40c95"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- configuration_internvl_chat.py\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_internvl_chat.py:   0%|          | 0.00/15.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01fac6ff96b04cc9b8e979e09b3bf07b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460d10fe1dbf4122a6bc4540b0c212dc"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_intern_vit.py:   0%|          | 0.00/18.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf65ed7e5e047aab8a089a454f865ca"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B:\n- modeling_internvl_chat.py\n- conversation.py\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"FlashAttention2 is not installed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/71.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91fd035b803e45608b10624e3f0b194c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a5bb01845543cc9a4ef7d3b8bbc2ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceb65e97c5054122948c364ee3fa8fa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698d8e310293495a923c246ddf3d36b3"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99dcd6108de842eab2d51cbadd9de66d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c49bf1ed1d574c9c983a720d589ef89a"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Конфиг LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],  # Пример для трансформеров, можно подстроить под модель\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:39:30.216128Z","iopub.execute_input":"2025-05-19T14:39:30.216446Z","iopub.status.idle":"2025-05-19T14:39:30.358726Z","shell.execute_reply.started":"2025-05-19T14:39:30.216416Z","shell.execute_reply":"2025-05-19T14:39:30.357885Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,686,400 || all params: 3,716,324,352 || trainable%: 0.0992\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Preparing the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(DATASET_NAME, DATASET_SUBDIR)\n\ntrain_data = dataset[\"train\"].select(range(3000))\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:39:30.359681Z","iopub.execute_input":"2025-05-19T14:39:30.359980Z","iopub.status.idle":"2025-05-19T14:44:41.808637Z","shell.execute_reply.started":"2025-05-19T14:39:30.359957Z","shell.execute_reply":"2025-05-19T14:44:41.807731Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/37.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c9839597a3482eb2bec3088e026f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9a1b82b5ba483b9657e60aea973437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/19 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3980674d0da64550814d4de62936cc5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00000-of-00019.parquet:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77d7da31174472984b7f77415211703"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00001-of-00019.parquet:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba73f12bd804a8f9b2544587a4563cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00002-of-00019.parquet:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad68cb4c64349d38fc7d901dcd5955d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00003-of-00019.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06561212b8ed4b09adf5f0d6e41075c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00004-of-00019.parquet:   0%|          | 0.00/112M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0896b0fb5e664478988057b432abbc2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00005-of-00019.parquet:   0%|          | 0.00/125M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3572dd7af10d4315bc1c2874bda0b140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00006-of-00019.parquet:   0%|          | 0.00/116M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd885b45b90f4db29bd44d3094d039ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00007-of-00019.parquet:   0%|          | 0.00/130M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57c50cca9e4b4bdf8e42da282e132d68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00008-of-00019.parquet:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0c5a4e1e3fc4364a0c9fddcbda55dac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00009-of-00019.parquet:   0%|          | 0.00/124M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58380b7c5704bb2a62a0aefbd915dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00010-of-00019.parquet:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0454bf697dc4482384b57c7e31b8b1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00011-of-00019.parquet:   0%|          | 0.00/128M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c02c2125fa74ba6a391ecb1e48bd933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00012-of-00019.parquet:   0%|          | 0.00/113M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576b174ea6ba4e4e9fedee2f7f31d3f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00013-of-00019.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a3e1454f7e46dab3c5c71ee0658de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00014-of-00019.parquet:   0%|          | 0.00/108M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a9594660a74015a738280f2f6fd3f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00015-of-00019.parquet:   0%|          | 0.00/119M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c70f839bf50a4fc6a9a9841112dabb31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00016-of-00019.parquet:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d403f8be134f07957011fb9ece6b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00017-of-00019.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75bbb4fca32f4ef980b1e651b4872d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ureader_cap/train-00018-of-00019.parquet:   0%|          | 0.00/109M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e4751d2f35646a69b1c3fae57e4b255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/91434 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34c248547654ecfb21df334f903c82d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f2aae544874badb6e18cbe786e0417"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:41.809830Z","iopub.execute_input":"2025-05-19T14:44:41.810154Z","iopub.status.idle":"2025-05-19T14:44:41.816564Z","shell.execute_reply.started":"2025-05-19T14:44:41.810134Z","shell.execute_reply":"2025-05-19T14:44:41.815796Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:41.817364Z","iopub.execute_input":"2025-05-19T14:44:41.817678Z","iopub.status.idle":"2025-05-19T14:44:44.045333Z","shell.execute_reply.started":"2025-05-19T14:44:41.817660Z","shell.execute_reply":"2025-05-19T14:44:44.044276Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:44.048931Z","iopub.execute_input":"2025-05-19T14:44:44.049198Z","iopub.status.idle":"2025-05-19T14:44:45.260199Z","shell.execute_reply.started":"2025-05-19T14:44:44.049180Z","shell.execute_reply":"2025-05-19T14:44:45.259278Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def load_image(image_file, input_size=448, max_num=12):\n    image = image_file.convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:45.261523Z","iopub.execute_input":"2025-05-19T14:44:45.261838Z","iopub.status.idle":"2025-05-19T14:44:46.393390Z","shell.execute_reply.started":"2025-05-19T14:44:45.261808Z","shell.execute_reply":"2025-05-19T14:44:46.392357Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    input_ids_list = []\n    attention_mask_list = []\n    pixel_values_list = []\n    labels_list = []\n\n    for convs, image in zip(examples[\"conversations\"], examples[\"image\"]):\n        full_text = \"\"\n        label_mask = []\n\n        for turn in convs:\n            if turn[\"from\"] == \"human\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [0] * len(tokenizer.tokenize(text + \"\\n\"))\n            elif turn[\"from\"] == \"gpt\":\n                text = turn[\"value\"]\n                full_text += text + \"\\n\"\n                label_mask += [1] * len(tokenizer.tokenize(text + \"\\n\"))\n\n        tokenized = tokenizer(\n            full_text,\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"  # Возвращаем тензоры сразу\n        )\n\n        input_ids = tokenized[\"input_ids\"].squeeze(0)        # [MAX_LENGTH]\n        attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n\n        label_mask = label_mask[:MAX_LENGTH]\n        label_mask += [0] * (MAX_LENGTH - len(label_mask))\n\n        labels = input_ids.clone().tolist()\n        for i, mask in enumerate(label_mask):\n            if mask == 0:\n                labels[i] = -100\n        labels = torch.tensor(labels)\n\n        pixel_values = load_image(image, input_size=INPUT_SIZE, max_num=MAX_NUM)\n\n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        pixel_values_list.append(pixel_values)\n        labels_list.append(labels)\n\n    # Конвертируем списки тензоров в батч-тензоры\n    batch_input_ids = torch.stack(input_ids_list)\n    batch_attention_mask = torch.stack(attention_mask_list)\n    # pixel_values — разной размерности по num_tiles, нельзя stack сразу, пусть остаются списком\n    batch_labels = torch.stack(labels_list)\n\n    return {\n        \"input_ids\": batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"pixel_values\": pixel_values_list,  # оставляем списком, паддим в collate_fn\n        \"labels\": batch_labels,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:46.395813Z","iopub.execute_input":"2025-05-19T14:44:46.396358Z","iopub.status.idle":"2025-05-19T14:44:47.274231Z","shell.execute_reply.started":"2025-05-19T14:44:46.396323Z","shell.execute_reply":"2025-05-19T14:44:47.273370Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Splitting into training and test samples","metadata":{}},{"cell_type":"code","source":"# Передаём конкретные названия колонок для удаления\nremove_columns = train_data.column_names\n\n# Применяем map\nprocessed_dataset = train_data.map(\n    preprocess_function,\n    batched=True,\n    batch_size=16,\n    remove_columns=remove_columns,\n    load_from_cache_file=False\n)\n\n# Разбиваем на train/val\nsplit = processed_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:44:47.274979Z","iopub.execute_input":"2025-05-19T14:44:47.275283Z","iopub.status.idle":"2025-05-19T14:53:34.629349Z","shell.execute_reply.started":"2025-05-19T14:44:47.275258Z","shell.execute_reply":"2025-05-19T14:53:34.628561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df73849af7f4360b18c6227e67bf2f9"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])         # теперь все тензоры одинакового размера\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    pixel_values_list = [item[\"pixel_values\"] for item in batch]\n    max_tiles = max(pv.size(0) for pv in pixel_values_list)\n\n    padded_pixel_values = []\n    for pv in pixel_values_list:\n        pad_len = max_tiles - pv.size(0)\n        if pad_len > 0:\n            pad_tensor = torch.zeros((pad_len, *pv.shape[1:]), dtype=pv.dtype)\n            padded = torch.cat([pv, pad_tensor], dim=0)\n        else:\n            padded = pv\n        padded_pixel_values.append(padded)\n    pixel_values = torch.stack(padded_pixel_values)\n\n    labels = torch.stack([item[\"labels\"] for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"pixel_values\": pixel_values,\n        \"labels\": labels\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:53:34.631247Z","iopub.execute_input":"2025-05-19T14:53:34.631518Z","iopub.status.idle":"2025-05-19T14:53:34.637238Z","shell.execute_reply.started":"2025-05-19T14:53:34.631492Z","shell.execute_reply":"2025-05-19T14:53:34.636611Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:53:34.638079Z","iopub.execute_input":"2025-05-19T14:53:34.638345Z","iopub.status.idle":"2025-05-19T14:53:34.660075Z","shell.execute_reply.started":"2025-05-19T14:53:34.638320Z","shell.execute_reply":"2025-05-19T14:53:34.659333Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Defining metrics","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nimport clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:53:34.660826Z","iopub.execute_input":"2025-05-19T14:53:34.661136Z","iopub.status.idle":"2025-05-19T14:53:34.877630Z","shell.execute_reply.started":"2025-05-19T14:53:34.661089Z","shell.execute_reply":"2025-05-19T14:53:34.877064Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"cider_scorer = Cider()\nspice_scorer = Spice()\nclip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:53:34.878325Z","iopub.execute_input":"2025-05-19T14:53:34.878521Z","iopub.status.idle":"2025-05-19T14:55:01.142707Z","shell.execute_reply.started":"2025-05-19T14:53:34.878506Z","shell.execute_reply":"2025-05-19T14:55:01.142116Z"}},"outputs":[{"name":"stdout","text":"Downloading stanford-corenlp-3.6.0 for SPICE ...\nProgress: 384.5M / 384.5M (100.0%)\nExtracting stanford-corenlp-3.6.0 ...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 59.0MiB/s]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Функция для вычисления CLIPScore\ndef compute_clip_score(cand_sentences, ref_sentences, batch_size=16):\n    scores = []\n    for i in range(0, len(cand_sentences), batch_size):\n        batch_cand = cand_sentences[i:i+batch_size]\n        batch_ref = ref_sentences[i:i+batch_size]\n        cand_inputs = clip.tokenize(batch_cand).to(DEVICE)\n        ref_inputs = clip.tokenize(batch_ref).to(DEVICE)\n        cand_feats = clip_model.encode_text(cand_inputs)\n        ref_feats = clip_model.encode_text(ref_inputs)\n        cand_feats = cand_feats / cand_feats.norm(dim=-1, keepdim=True)\n        ref_feats = ref_feats / ref_feats.norm(dim=-1, keepdim=True)\n        scores.extend((cand_feats * ref_feats).sum(dim=-1).tolist())\n    return np.mean(scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:01.143595Z","iopub.execute_input":"2025-05-19T14:55:01.143828Z","iopub.status.idle":"2025-05-19T14:55:01.149556Z","shell.execute_reply.started":"2025-05-19T14:55:01.143809Z","shell.execute_reply":"2025-05-19T14:55:01.148874Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Optimizer, scheduler, accelerator","metadata":{}},{"cell_type":"code","source":"# --- Оптимизатор и scheduler ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nnum_training_steps = EPOCHS * len(train_loader)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# --- Accelerator для удобства ---\naccelerator = Accelerator()\nmodel, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_loader, val_loader, lr_scheduler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:01.150359Z","iopub.execute_input":"2025-05-19T14:55:01.151057Z","iopub.status.idle":"2025-05-19T14:55:01.309718Z","shell.execute_reply.started":"2025-05-19T14:55:01.151031Z","shell.execute_reply":"2025-05-19T14:55:01.309151Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Final preparations","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# --- Функция генерации для оценки (beam search для валид)\ndef generate_text(input_ids, attention_mask):\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=MAX_LENGTH,\n        num_beams=1,\n        early_stopping=True,\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:01.310401Z","iopub.execute_input":"2025-05-19T14:55:01.310678Z","iopub.status.idle":"2025-05-19T14:55:01.314946Z","shell.execute_reply.started":"2025-05-19T14:55:01.310654Z","shell.execute_reply":"2025-05-19T14:55:01.314012Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# --- Тренировочный цикл ---\ntrain_losses = []\nval_losses = []\nspice_scores = []\ncider_scores = []\nclip_scores = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:01.315685Z","iopub.execute_input":"2025-05-19T14:55:01.316430Z","iopub.status.idle":"2025-05-19T14:55:01.330593Z","shell.execute_reply.started":"2025-05-19T14:55:01.316411Z","shell.execute_reply":"2025-05-19T14:55:01.329904Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader):\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    # Валидация\n    model.eval()\n    total_val_loss = 0\n    preds = []\n    refs = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            # Генерируем предсказания для метрик\n            generated_texts = generate_text(batch[\"input_ids\"], batch[\"attention_mask\"])\n            refs.extend(tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True))\n            preds.extend(generated_texts)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    # Подсчет SPICE и CIDEr (pycocoevalcap работает со словарями)\n    # Формат: [{'image_id': i, 'caption': 'text'}, ...]\n    res = [{\"image_id\": i, \"caption\": p} for i, p in enumerate(preds)]\n    gts = {i: [refs[i]] for i in range(len(refs))}\n\n    cider_score, _ = cider_scorer.compute_score(gts, res)\n    spice_score, _ = spice_scorer.compute_score(gts, res)\n\n    cider_scores.append(cider_score)\n    spice_scores.append(spice_score)\n\n    # CLIPScore\n    clip_score = compute_clip_score(preds, refs)\n    clip_scores.append(clip_score)\n\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | SPICE: {spice_score:.4f} | CIDEr: {cider_score:.4f} | CLIPScore: {clip_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:01.331404Z","iopub.execute_input":"2025-05-19T14:55:01.331667Z","iopub.status.idle":"2025-05-19T14:55:03.456682Z","shell.execute_reply.started":"2025-05-19T14:55:01.331615Z","shell.execute_reply":"2025-05-19T14:55:03.455613Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/2700 [00:02<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1814084295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/376399420.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# теперь все тензоры одинакового размера\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpixel_values_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_tiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpixel_values_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"],"ename":"TypeError","evalue":"expected Tensor as element 0 in argument 0, but got list","output_type":"error"}],"execution_count":23},{"cell_type":"markdown","source":"### Saving model","metadata":{}},{"cell_type":"code","source":"# --- Сохраняем модель ---\nmodel.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Модель сохранена в {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:03.457174Z","iopub.status.idle":"2025-05-19T14:55:03.457396Z","shell.execute_reply.started":"2025-05-19T14:55:03.457287Z","shell.execute_reply":"2025-05-19T14:55:03.457296Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graphical visualization","metadata":{}},{"cell_type":"code","source":"epochs = np.arange(1, EPOCHS+1)\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, spice_scores, label=\"SPICE\", color=\"orange\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"SPICE\")\n\nplt.subplot(2, 2, 3)\nplt.plot(epochs, cider_scores, label=\"CIDEr\", color=\"green\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CIDEr\")\n\nplt.subplot(2, 2, 4)\nplt.plot(epochs, clip_scores, label=\"CLIPScore\", color=\"red\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"CLIPScore\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T14:55:03.458845Z","iopub.status.idle":"2025-05-19T14:55:03.459156Z","shell.execute_reply.started":"2025-05-19T14:55:03.459001Z","shell.execute_reply":"2025-05-19T14:55:03.459011Z"}},"outputs":[],"execution_count":null}]}