{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Installing requirements"
      ],
      "metadata": {
        "id": "m86DD_e8CWt1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMldH_RGA2Rh"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade accelerate\n",
        "!pip install --upgrade bitsandbytes\n",
        "!pip install --upgrade requests\n",
        "!pip install --upgrade pillow\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install torch==2.6.0\n",
        "!pip install torchvision\n",
        "!pip install fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model loading"
      ],
      "metadata": {
        "id": "HxC16EXvCfol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "-fDb_mK6BEZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"./finetuned_model\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    trust_remote_code=True\n",
        ").eval()"
      ],
      "metadata": {
        "id": "rbeQpUdaBJRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialization of functions for preprocessing"
      ],
      "metadata": {
        "id": "9GYb2R1VCtVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "YkTafjxJBK7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageNet Normalization\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ],
      "metadata": {
        "id": "7OnT1HbCBQ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transform(input_size=448):\n",
        "    '''Pipeline for transformation'''\n",
        "    return T.Compose([\n",
        "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "def preprocess_image(image: Image.Image, image_size=448):\n",
        "    '''Image transformation'''\n",
        "    image = image.convert('RGB')\n",
        "    transform = build_transform(image_size)\n",
        "    pixel_values = transform(image).unsqueeze(0)\n",
        "    return pixel_values"
      ],
      "metadata": {
        "id": "iucftS32BSgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference"
      ],
      "metadata": {
        "id": "I0JNFb8hCwxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dybjNXXqBUmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
        "generation_config = dict(max_new_tokens=128, do_sample=False)\n",
        "\n",
        "# Image loading\n",
        "image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "# Image display\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Preprocessing\n",
        "pixel_values = preprocess_image(image).to(dtype=torch.bfloat16, device=model.device)\n",
        "\n",
        "# Text input to model\n",
        "additional_text = \"urban life in Beijing\"\n",
        "\n",
        "# Prompt\n",
        "question = (\n",
        "    \"<image>\\n\"\n",
        "    \"Please describe the visual content of the image, \"\n",
        "    \"and provide a short paragraph that connects the image to \"\n",
        "    f\"the following text concept: '{additional_text}'.\"\n",
        ")\n",
        "\n",
        "# Model inference\n",
        "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
        "\n",
        "print(\"Question:\\n\" + question.replace('<image>', '[IMAGE]'))\n",
        "print(\"Answer:\\n\" + response)"
      ],
      "metadata": {
        "id": "Klq8q_T5BXI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}