{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tuning language part of model","metadata":{}},{"cell_type":"markdown","source":"### Installing requirements","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade \\\n    pip \\\n    setuptools \\\n    wheel\n!pip install -q \\\n    torch \\\n    numpy \\\n    tqdm \\\n    \"transformers>=4.41.0\" \\\n    \"huggingface_hub>=0.23.2\" \\\n    peft \\\n    accelerate \\\n    matplotlib \\\n    git+https://github.com/tingofurro/summac \\\n    \"datasets<=2.14.6\"\n!pip install -q --upgrade bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Necessary imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_scheduler, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\nfrom accelerate import Accelerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining variables","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"OpenGVLab/InternVL2_5-4B\"\nDATASET_NAME = \"RussianNLP/Mixed-Summarization-Dataset\"\nOUTPUT_DIR = \"./finetuned_model\"\nBATCH_SIZE = 2\nEPOCHS = 3\nLR = 2e-4  # learning rate\nMAX_LENGTH = 128\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Uploading model and tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# Model 4bit с bnb\nfull_model = AutoModel.from_pretrained(\n    MODEL_NAME,\n    device_map='auto',\n    quantization_config=quantization_config,\n    trust_remote_code=True\n)\n\n# Connecting language part of model\nmodel = full_model.language_model\n# Preparing model for QLoRA\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LoRA configuration","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # for LLMs\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the dataset","metadata":{}},{"cell_type":"code","source":"train_data = load_dataset(DATASET_NAME, split='train')\n\ntrain_data = train_data.select(range(10000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = examples[\"text\"]\n    targets = examples[\"summary\"]\n    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)  # no return_tensors=\"pt\"\n    labels = tokenizer(targets, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)[\"input_ids\"]\n    # replace pad tokens with -100 in labels (for ignoring in loss)\n    labels = [[(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels]\n    model_inputs[\"labels\"] = labels\n    return model_inputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_dataset = train_data.map(\n    preprocess_function,\n    batched=True,\n    batch_size=16,\n    remove_columns=train_data.column_names,\n    load_from_cache_file=False\n)\n\n# Splitting on training/validation\nsplit = processed_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split[\"train\"]\nval_dataset = split[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Collating","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = torch.tensor([sample['input_ids'] for sample in batch], dtype=torch.long)\n    attention_mask = torch.tensor([sample['attention_mask'] for sample in batch], dtype=torch.long)\n    labels = torch.tensor([sample['labels'] for sample in batch], dtype=torch.long)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': labels\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Batching","metadata":{}},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Defining metrics","metadata":{}},{"cell_type":"code","source":"from summac.model_summac import SummaCConv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summac_model = SummaCConv(\n    granularity=\"sentence\",\n    models=[\"vitc\"],\n    device=DEVICE,\n    start_file=None,\n    use_con=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\nsummac_scores = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimizer, scheduler, accelerator","metadata":{}},{"cell_type":"code","source":"# Optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nnum_training_steps = EPOCHS * len(train_loader)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# Accelerator\naccelerator = Accelerator(mixed_precision=\"fp16\")\nmodel, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_loader, val_loader, lr_scheduler\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final preparations","metadata":{}},{"cell_type":"markdown","source":"### Generating text for validation","metadata":{}},{"cell_type":"code","source":"def generate_text(input_ids, attention_mask):\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_new_tokens=32,\n        num_beams=1\n    )\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader):\n        batch = {k: v.to(model.device) for k, v in batch.items()}\n    \n        outputs = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    model.eval()\n    total_val_loss = 0\n    preds = []\n    sources = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            batch = {k: v.to(model.device) for k, v in batch.items()}\n    \n            outputs = model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                labels=batch[\"labels\"]\n            )\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            generated_texts = generate_text(batch[\"input_ids\"], batch[\"attention_mask\"])\n            preds.extend(generated_texts)\n            for input_id in batch[\"input_ids\"]:\n                sources.append(tokenizer.decode(input_id, skip_special_tokens=True))\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    summac_result = summac_model.score(sources, preds, batch_size=8)\n    summac_score = np.mean(summac_result[\"scores\"])\n    summac_scores.append(summac_score)\n\n    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | SummaC: {summac_score:.4f})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Saving model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Model is saved: {OUTPUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graphical visualization","metadata":{}},{"cell_type":"code","source":"epochs = np.arange(1, EPOCHS+1)\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 2, 1)\nplt.plot(epochs, train_losses, label=\"Train Loss\")\nplt.plot(epochs, val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.title(\"Loss\")\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs, summac_scores, label=\"SummaC\", color=\"orange\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Score\")\nplt.legend()\nplt.title(\"SummaC\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}